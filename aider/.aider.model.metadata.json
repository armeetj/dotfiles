{
        "sample_spec": {
                "max_tokens": "LEGACY parameter. set to max_output_tokens if provider specifies it. IF not set to max_input_tokens, if provider specifies it.",
                "max_input_tokens": "max input tokens, if the provider specifies it. if not default to max_tokens",
                "max_output_tokens": "max output tokens, if the provider specifies it. if not default to max_tokens",
                "input_cost_per_token": 0,
                "output_cost_per_token": 0,
                "litellm_provider": "one of https://docs.litellm.ai/docs/providers",
                "mode": "one of chat, embedding, completion, image_generation, audio_transcription, audio_speech",
                "supports_function_calling": true,
                "supports_parallel_function_calling": true,
                "supports_vision": true,
                "supports_audio_input": true,
                "supports_audio_output": true,
                "supports_prompt_caching": true,
                "supports_response_schema": true,
                "supports_system_messages": true,
                "deprecation_date": "date when the model becomes deprecated in the format YYYY-MM-DD"
        },
        "openai/gpt-3.5-turbo": {
                "max_input_tokens": 12288,
                "max_output_tokens": 4096,
                "input_cost_per_token": 0,
                "output_cost_per_token": 0,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": false,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": false,
                "supports_system_messages": true
        },
        "openai/gpt-3.5-turbo-0613": {
                "max_input_tokens": 12288,
                "max_output_tokens": 4096,
                "input_cost_per_token": 0,
                "output_cost_per_token": 0,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": false,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": false,
                "supports_system_messages": true
        },
        "openai/gpt-4": {
                "max_input_tokens": 32768,
                "max_output_tokens": 4096,
                "input_cost_per_token": 0,
                "output_cost_per_token": 0,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": false,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": false,
                "supports_system_messages": true
        },
        "openai/gpt-4-0613": {
                "max_input_tokens": 32768,
                "max_output_tokens": 4096,
                "input_cost_per_token": 0,
                "output_cost_per_token": 0,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": false,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": false,
                "supports_system_messages": true
        },
        "openai/gpt-4o": {
                "max_input_tokens": 128000,
                "max_output_tokens": 4096,
                "input_cost_per_token": 0.0000025,
                "output_cost_per_token": 0.00001,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": true,
                "supports_vision": true,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": false,
                "supports_system_messages": true
        },
        "openai/gpt-4o-2024-05-13": {
                "max_input_tokens": 128000,
                "max_output_tokens": 4096,
                "input_cost_per_token": 0,
                "output_cost_per_token": 0,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": true,
                "supports_vision": true,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": false,
                "supports_system_messages": true
        },
        "openai/gpt-4-o-preview": {
                "max_input_tokens": 64000,
                "max_output_tokens": 4096,
                "input_cost_per_token": 0,
                "output_cost_per_token": 0,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": true,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": false,
                "supports_system_messages": true
        },
        "openai/gpt-4o-2024-08-06": {
                "max_input_tokens": 64000,
                "max_output_tokens": 16384,
                "input_cost_per_token": 0,
                "output_cost_per_token": 0,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": true,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": false,
                "supports_system_messages": true
        },
        "openai/gpt-4o-mini": {
                "max_input_tokens": 12288,
                "max_output_tokens": 4096,
                "input_cost_per_token": 1.5e-7,
                "output_cost_per_token": 6e-7,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": true,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": false,
                "supports_system_messages": true
        },
        "openai/gpt-4o-mini-2024-07-18": {
                "max_input_tokens": 12288,
                "max_output_tokens": 4096,
                "input_cost_per_token": 0,
                "output_cost_per_token": 0,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": true,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": false,
                "supports_system_messages": true
        },
        "openai/o1": {
                "max_input_tokens": 20000,
                "input_cost_per_token": 0.000015,
                "output_cost_per_token": 0.00006,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": false,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": true,
                "supports_system_messages": true
        },
        "openai/o1-2024-12-17": {
                "max_input_tokens": 20000,
                "input_cost_per_token": 0,
                "output_cost_per_token": 0,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": false,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": true,
                "supports_system_messages": true
        },
        "openai/o3-mini": {
                "max_input_tokens": 64000,
                "max_output_tokens": 100000,
                "input_cost_per_token": 0.0000025,
                "output_cost_per_token": 0.00001,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": false,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": true,
                "supports_system_messages": true
        },
        "openai/o3-mini-2025-01-31": {
                "max_input_tokens": 64000,
                "max_output_tokens": 100000,
                "input_cost_per_token": 0,
                "output_cost_per_token": 0,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": false,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": true,
                "supports_system_messages": true
        },
        "openai/o3-mini-paygo": {
                "max_input_tokens": 64000,
                "max_output_tokens": 100000,
                "input_cost_per_token": 0,
                "output_cost_per_token": 0,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": false,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": true,
                "supports_system_messages": true
        },
        "openai/claude-3.5-sonnet": {
                "max_input_tokens": 90000,
                "max_output_tokens": 8192,
                "input_cost_per_token": 0.000003,
                "output_cost_per_token": 0.000015,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": true,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": false,
                "supports_system_messages": true
        },
        "openai/claude-3.7-sonnet": {
                "max_input_tokens": 90000,
                "max_output_tokens": 8192,
                "input_cost_per_token": 0.000003,
                "output_cost_per_token": 0.000015,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": true,
                "supports_parallel_function_calling": true,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": false,
                "supports_system_messages": true
        },
        "openai/claude-3.7-sonnet-thought": {
                "max_input_tokens": 90000,
                "max_output_tokens": 8192,
                "input_cost_per_token": 0.000003,
                "output_cost_per_token": 0.000015,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": false,
                "supports_parallel_function_calling": false,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": false,
                "supports_system_messages": true
        },
        "openai/gemini-2.0-flash-001": {
                "max_input_tokens": 128000,
                "max_output_tokens": 8192,
                "input_cost_per_token": 1e-7,
                "output_cost_per_token": 4e-7,
                "litellm_provider": "openai",
                "mode": "chat",
                "supports_function_calling": false,
                "supports_parallel_function_calling": false,
                "supports_vision": false,
                "supports_audio_input": false,
                "supports_audio_output": false,
                "supports_prompt_caching": false,
                "supports_response_schema": false,
                "supports_system_messages": true
        }
}
